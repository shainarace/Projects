--- 
title: "21,578 Reuters' Newswires"
author: "Shaina Race Bennett, PhD"
output: html_document
site: bookdown::bookdown_site
github-repo: shainarace/Reuters
---

# Prepare Data

Key Takeaways:

1. File `reut2-017.sgm` is not UTF-8 and needs to be saved as UTF-8 for life to be easy. 
2. This legacy `.sgm` file extension which is essentially poorly-formed .xml picked fights with every parser that it encountered, even Beautiful Soup. 
3. Reading the corpus and parsing the tags manually turned out to be the most efficient solution.
4. Two of the <DATE> fields (out of 21,578) required re-formatting: ` 5-APR-1987 01:53:30` and `31-MAR-1987 605:12:1`. 

---

These 21,578 documents are readily available in an R document corpus in the package `tm.corpus.Reuters21578`. However, this corpus comes pre-loaded with many missing headings and _mostly_ missing dates, so we read the files line-by-line to extract the date, text (with heading), and the value of `lewissplit` in case we wanted to perform some predictive modeling. This ended up being really annoying as the result of __non-UTF-8 encoding on file 17__ - but once that was fixed, we got what we needed.

Beautiful Soup was no help here. 

```{r message=F, warning=F}
# install.packages("tm.corpus.Reuters21578", repos = "http://datacube.wu.ac.at")
# install.packages("SnowballC")
# install.packages("textreg")
# install.packages('tm.corpus.Reuters21578')
# install.packages('text2vec')
# install.packages('Matrix')
# install.packages('umap')
# install.packages('tm')
# install.packages('slam')
# install.packages('irlba')
# install.packages('dbscan')
# install.packages('plotly')
# install.packages('gridExtra')
# install.packages('lubridate')
# install.packages('maxmatching')
# install.packages('plyr')
# install.packages('rARPACK')
# install.packages('textrank')
# install.packages('rvest')
# install.packages('tidytext')
# install.packages('tsne')

# ##################################################################
library(textrank)
library(rvest)
library(tidytext)
library(dbscan)
library(ggplot2)
library(irlba)
library(uwot)
library(slam)
library(text2vec)
#library(tm.corpus.Reuters21578)
library(SnowballC)
library(tm)
library(textreg)
library(stringr)
library(Matrix)
library(plotly)
library(gridExtra)
library(lubridate)
library(rARPACK)
library(htmlwidgets)
library(bookdown)
library(stringr)
library(fs)
library(tsne)
```

```{r message=F, warning=F}
#setwd('/Users/shaina/Library/Mobile Documents/com~apple~Clouddocs/final_data_plots/Datasets and Code/reuters21578/')
# 
# PATH = '/Users/shaina/Library/Mobile Documents/com~apple~Clouddocs/Datasets and Code/reuters21578/Files/'
# filenames=dir_ls(PATH)
# datetime=vector()
# text = vector()
# lewissplit=vector()
# 
# 
# for(file in filenames){
#   con = file(file,"r", encoding = "UTF-8")
#   line = readLines(con, encoding="UTF-8")
#   line = paste(line, sep=' ', collapse = ' ')
#   # String size fixed
#   lewis_idx=str_locate_all(pattern='LEWISSPLIT=',line)
#   date_idx=str_locate_all(pattern='<DATE>',line)
#   for(i in 1:nrow(lewis_idx[[1]])){
#     datetime[length(datetime)+1]=substr(line, date_idx[[1]][i,2]+1,date_idx[[1]][i,2]+20)
#     lewissplit[length(lewissplit)+1]=substr(line, lewis_idx[[1]][i,2]+2,lewis_idx[[1]][i,2]+5)
#   }
#   # String size not fixed
#   line = paste(line, sep=' ', collapse = ' ')
#   line=gsub('</','<',line)
#   text_idx=str_locate_all(pattern='<TEXT',line)
#   for(i in seq(1,nrow(text_idx[[1]])-1,2)){
#     text[length(text)+1]=substr(line, text_idx[[1]][i,2]+2,text_idx[[1]][i+1,1]-1)
#   }
#   close(con)
# }
# 
# # Annoying that I end up with fewer documents than I should. Investigate that.
# 
# # Explore why I end up with fewer documents than I should by counting occurrences of <TEXT on each file: <aha> # Fixed UTF-8 encoding on file 17 and now it works.
# date = as.POSIXct(datetime,format = '%d-%b-%Y %H:%M:%S')
# datetime[which(is.na(date))]
# datetime[which(is.na(date))] = c("5-APR-1987 01:53:30", "31-MAR-1987 05:12:1")
# datetime = as.POSIXct(datetime,format = '%d-%b-%Y %H:%M:%S')
# lewissplit[lewissplit=='TRAI']='train'
# lewissplit[lewissplit=='TEST']='test'
# #
# # ############################################################################
# # # Now I can pull out the heading...
# # ############################################################################
# title_idx = str_locate_all(pattern='<TITLE>',text)
# head=vector()
# for(i in 1:(length(text))){
#   if(nrow(title_idx[[i]])<2){
#     head[i]=''
#   }else{
#       head[i]=substr(text[i], title_idx[[i]][1,2]+1,title_idx[[i]][2,1]-1)
#     }
# }
# head=gsub('&lt;','<',head, fixed=T)
# # ############################################################################
# # # ...and the raw article text.
# # ############################################################################
# body_idx = str_locate_all(pattern='<BODY>',text)
# body=vector()
# for(i in 1:(length(text))){
#   if(nrow(body_idx[[i]])<2){
#     body[i]=text[i]
#   }else{
#       body[i]=substr(text[i], body_idx[[i]][1,2]+1,body_idx[[i]][2,1]-1)
#     }
# }
# # Clean up Briefs
# body = gsub("TYPE=\"BRIEF\">&#2; ******<TITLE>",'',body, fixed=T)
# body = gsub("<TITLE>Blah blah blah. &#3;", '',body, fixed=T)
# 
# 
#  text=body
#  Reuters <- Corpus(VectorSource(text))
# save(text,head,Reuters,lewissplit,datetime,file='docs/final_data_plots/RawDataRead.RData')
 load('docs/final_data_plots/RawDataRead.RData')
```



1. Make lower case, emove stop words + "Reuters", punctuation, and numbers; Employ stemming.
2. Create binary term-document matrix to remove terms occurring in less than 5 documents.
5. Remove documents left with fewer than 10 words remaining.
6. Subset the datetime, topics, and heading information accordingly.


```{r}
# load('Reuters.RData')

# # 1
# ############################################################
# R = Reuters
# R = tm_map(R,content_transformer(tolower))
# R = tm_map(R,removeWords,stopwords("en"))
# R = tm_map(R,removePunctuation)
# R = tm_map(R,removeNumbers)
# R = tm_map(R,stemDocument)
# R = tm_map(R,removeWords, c('reuter', 'dlrs', 'mln', 'said','will', 'year', 'compani','pct','corp' ))
# # ############################################################
# # 2
# # ############################################################
# tdm = TermDocumentMatrix(R)
# binary = weightBin(tdm)
# keep_terms = row_sums(binary)>=5
# tdm = tdm[keep_terms,]
# # ############################################################
# # 3
# # ############################################################
# keep_docs = col_sums(tdm)>10
# R = R[keep_docs]
# tdm = tdm[,keep_docs ]
# dim(tdm)
# length(R)
# # ############################################################
# # 4
# # ############################################################
# datetime = datetime[keep_docs]
# lewissplit=lewissplit[keep_docs]
# head=head[keep_docs]
# raw_text=text[keep_docs]
# #############################################################
# # add breaks for text wrapping
# #############################################################
# raw_text = gsub("(.{60,}?)\\s", "\\1<br>", raw_text)
# # ############################################################
# # Save data to avoid repeat processing
# # ############################################################
# save(raw_text,head,lewissplit,tdm,R,datetime, file='docs/final_data_plots/processedV2.RData')
load('docs/final_data_plots/processedV2.RData')
```

# Exploratory Analysis 

Key Takeaways:

1. Some terms are _extraordinarily_ frequent in this corpus. The following words belong on our stoplist for this project. We could likely grow this list, but it's a good start: `'reuter', 'dlrs', 'mln', 'said','will', 'year', 'compani','pct','corp'` 
2. Not all documents in this corpus are created equal. There appear to be a mix of some type labeled "briefs" and other more traditional article-like newswires. The briefs don't appear to have any text at all, but they have titles - They have no field tagged as `<BODY>` and the `<TEXT>` field only contains the title - so we have to clean that up in our initial processing.
3. Huge skew on document length. Normalization will surely help.

---

## Term Frequencies

This initial exploration of term frequencies allowed us to find some extraordinarily common words for this corpus that were added to the stop list. Not _all_ common words were added to the stop list, however. Some common words may still have found relevance in determining certain groups of documents (an example would be the abbreviation 'vs' which indicates a comparison, most notably from an earnings report.)

### Documents per word {-}

```{r message=F, warning=F}
bin = weightBin(tdm)
df=data.frame(docFreqs = row_sums(bin))
g1 = ggplot(df,aes(x=docFreqs)) + 
    geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) +
    labs(x = "Number of Documents in which \n a Word Appears", y="Frequency")  
df=data.frame(docFreqs=df$docFreqs[df$docFreqs<100] )
g2 =  ggplot(df,aes(x=docFreqs)) + 
    geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) +
    labs(x = "Number of Documents in which \n a Word Appears", y='Frequency',
    title='Same Distribution Cut at x=100')
grid.arrange(g1,g2,ncol=2)
```

### Words per doc {-}

```{r message=F, warning=F}
bin = weightBin(tdm)
df=data.frame(docSums = col_sums(bin))
g1 = ggplot(df,aes(x=docSums)) + 
    geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) +
    labs(x = "Number of Terms", y="Frequency")  
g1
```

### TF-IDF per word {-}

```{r message=F, warning=F}
tfi = weightTfIdf(tdm)
df = data.frame(termFreqs = row_sums(tfi))

g1 = ggplot(df,aes(x=termFreqs)) + 
    geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) +
    labs(x = "Sum of TF-IDF Weights \n for each words")  
df=data.frame(termFreqs=df$termFreqs[df$termFreqs<100] )
g2 =  ggplot(df,aes(x=termFreqs)) + 
    geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) +
    labs(x = "Sum of TF-IDF Weights \n for each word", y='Frequency',
    title='Same Distribution Cut at x=100')
grid.arrange(g1,g2,ncol=2)

```

## via SVD

Try to get an idea of the meaningful dimensionality of the data with a screeplot.

### Screeplot {-}

```{r message=F, warning=F}
# tfidf_tdm = weightTfIdf(tdm, normalize=T)
# # # MUST NORMALIZE THOSE DOCUMENT LENGTHS!!
# tfidf_tdm = apply(tfidf_tdm, 2, function(x){x/c(sqrt(t(x)%*%x))})
# tfidf_tdm <- as(tfidf_tdm, "sparseMatrix")
# svd = irlba(tfidf_tdm, 150)
# save(svd,tfidf_tdm,file='docs/final_data_plots/svd.RData')
load('docs/final_data_plots/svd.RData')
df = data.frame(x=1:150,d=svd$d)
g1 = ggplot(data=df, aes(x=x, y=d, group=1)) +
  geom_line(color="red")+labs(y='Singular Values',x='index', 
                              title='Screeplot of Reuters tf-idf Matrix, vlines at 10, 25') + 
  geom_point() + 
  geom_vline(xintercept = 25, linetype="dotted",  color = "blue", size=1) + 
  geom_vline(xintercept = 10, linetype="dotted", color = "blue", size=1)
u.df = data.frame(x=svd$v[,1], y=svd$v[,2])
 
g1
```

### 2-D Projection {-}

Our [initial Creation of this SVD Projection](https://shainarace.github.io/Reuters/SVD_shows_briefs_vs_articles.html) allowed us to see that we had an issue with _briefs_ vs. _articles_ in this data, something that may have caused noise in subsequent analysis had we not been careful about the exploratory phase. 

```{r,eval=F}
fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = svd$v[,1],
    y = svd$v[,2],
    text = ~paste('heading:', head ,"<br>text: ", raw_text  ),
    hoverinfo = 'text',
    marker = list(color='green', opacity=0.6),
    showlegend = F
  )

# saveWidget(fig,file='docs/svd_projection.html')
```     


[Full Page Visualization](https://shainarace.github.io/Reuters/svd_projection.html)

# UMAP

Key Takeaways:

1. UMAP is fantastic for visual dimension reduction.
2. The UMAP visualizations with GloVe vector input were not as sharp as those with singular vector input. 
3. Using 15-25 singular vectors as suggested by the screeplot made a fine visualization. However, increasing the number to 150 seemed to help (qualitatively speaking) and did not take much longer, so we stuck with 150. 
4. One can easily see how this 2D approximation provides nice inputs for any predictive topic model - UMAP _does_ have a function to project new data onto the space - we could easily watch new data coming in be filtered away into groups like `earnings reports`, `government data releases`, `dividend announcements` etc. 

---

We'll use the mathemagical [Uniform Manifold Approximation and Projection (UMAP)](https://umap-learn.readthedocs.io/en/latest/) algorithm to project the already dimension-reduced data (150 singular vectors) into 2-space. UMAP is a dimension reduction technique that builds on the notion neighbor graphs with ideas from topology. It is similar to t-SNE in its approach, but the fundamentals are based on firmer (and more complicated) mathematical theory (manifolds/topology).  

```{r message=F, warning=F}
#svd_ump = umap(svd$v[,1:150])
#save(svd_ump, file='docs/final_data_plots/svd_ump.RData')
load('docs/final_data_plots/svd_ump.RData')

fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = svd_ump[,1],
    y = svd_ump[,2],
    text = ~paste('heading:', head ,"<br>text: ", raw_text  ),
    hoverinfo = 'text',
    marker = list(color='green', opacity=0.6),
    showlegend = F
  )
# saveWidget(fig, "docs/UMAP_noClusters.html")
```


[Full Page Visualization](https://shainarace.github.io/Reuters/UMAP_noClusters.html)


# Clustering: HDBSCAN 

Key Takeaways:

1. Clustering 20,000 documents is never going to be clean. HDBSCAN makes a nice algorithm for the task because it will find dense clusters of points amidst a sea of noise - this means it identifies points that don't belong to an area of higher density and labels them as noise. Are they necessarily noise? No. Should we _ignore_ these documents altogether? _No_. But we might ought to treat them differently from the denser regions that are more clearly clusters. Maybe noise points need a fuzzy treatment where they are compared to nearby clusters and given scores that measure the extent to which they belong in each nearby cluster. 
2. Getting a manageable number of clusters is unlikely to be helpful. A cluster like "data reports" might contain earnings releases, foreign debt reports, government employment reports etc - broad topics provide no specificity overall. 
3. Getting a thousand clusters with no idea of how those clusters are similar to each other is also unhelpful. In this situation, topics are just overly specific islands.
4. Ideally we want many specific clusters that cluster together into larger topics. Like the "data reports" example given earlier: if we could have all of the _subtopics_ (earnings reports, foreign debt reports, government employment reports etc) and know they fall into that broader category of "data reports", then we've found a nice organization of the corpus. HDBSCAN is a typically a good tool for this task. 
5. Result of clustering the UMAP projection was nicer than clustering in the SVD space and clustering in the GloVe space.

---

To get clusters, we consider just 2 options here - can cluster this UMAP projection or can opt to cluster some higher-dimensional projection (like the singular vectors themselves) and see how that looks in the UMAP Space. UMAP clustering seems to perform _really_ well, even better than singular vector input, so we stick with it. We define `n` as the number of documents and `k` as the number of clusters.

Hierarchical DBSCAN is a fast algorithm that adapts the ideas of single linkage clustering (minimal spanning trees) to [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) (density based spatial clustering of applications with noise) to create a hierarchical map of density based clusters. 

```{r message=F, warning=F}
 # clus = hdbscan(svd_ump,6)
# save(clus,file='docs/final_data_plots/alldocs_hdbscan_of_map6.RData')
load('docs/final_data_plots/alldocs_hdbscan_of_map6.RData')
n=length(clus$cluster)
(k = length(clus$cluster_scores))
```

We get a LOT of clusters from `hdbscan` - this makes sense, there is a lot going on in this corpus! But it might be nice to refine those clusters so that we can see which ones are related. We'll get to that after we explore this great visualization. 

# The Grand Visualization

__Note: We don't have enough colors! The colors are recycled but hopefully will still help. Cluster numbers in tooltip for certainty__

```{r message=F, warning=F, eval=F}
clusters = factor(clus$cluster)

fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = svd_ump[,1],
    y = svd_ump[,2],
    text = ~paste('Heading:', head ,"<br>Text: ", raw_text ,"<br>Cluster Number: ", clusters),
    hoverinfo = 'text',
    color=clusters,
    marker = list( opacity=0.6),
    showlegend = F
  )

# saveWidget(fig, "docs/All_clusters_noTopics_UMAPClus_wNoise.html")
```

[Full Page Visualization](https://shainarace.github.io/Reuters/All_clusters_noTopics_UMAPClus_wNoise.html)


## Omit some noise points for more cluster clarity

We can reduce the noise on the plot by omitting some of the points with high outlier scores; generally I hate doing this because it can be a good way to accidently lose something you didn't know you wanted. However, it could have it's advantages as a strategy and the `outlier_score` of `hdbscan()` is a nice threshold to play with for further analytical paths. 


```{r message=F, warning=F, warning=F, eval=F}
index_subset = clus$outlier_scores<0.6
data_subset = svd_ump[index_subset,]
raw_text_subset = raw_text[index_subset]
head_subset = head[index_subset]
clusters = factor(clus$cluster[index_subset])

fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = data_subset[,1],
    y = data_subset[,2],
    text = ~paste('Heading:', head_subset ,"<br>Text: ", raw_text_subset ,"<br>Cluster Number: ", clusters),
    hoverinfo = 'text',
    color = clusters,
    marker = list(opacity=0.6),
    showlegend = F
  )
# saveWidget(fig, "docs/All_clusters_noTopics_UMAPClus.html")

```

[Full Page Visualization](https://shainarace.github.io/Reuters/All_clusters_noTopics_UMAPClus.html)


# Cluster Refinement

Problem with our initial clustering: Many medium-big clusters get broken up even though they seem nicely separated on the UMAP projection. \

Two ideas for potential refinement:\
1. Run the clustering again on centroids of clusters\
2. __Take all the existing clusters as single documents (i.e. paste all documents in a cluster together into single document for each cluster) -- repeat this process.__ I think this is a nice idea but I don't believe I'll have time to play with it. \

The first idea is easier so we'll start there:

## Refinement Idea 1: Clustering the centroids

### Topic Keywords for Clusters {-}

Get top words for further visualization and pile all documents in a cluster into one giant document for the purposes of summarization.

```{r message=F, warning=F}
top.words=list()
cluster.docs = vector()
centroids = matrix(NA,k,2)
mem=matrix(NA,nrow=n,ncol=k)

for(i in 1:k){
  mem[,i] = clus$cluster ==i
  tdmi = tfidf_tdm[,mem[,i]]
  rs = row_sums(tdmi)
  top.words[[i]] = names(rs[order(rs,decreasing=T)])[1:10]
  cluster.docs[i] = paste(raw_text[clus$cluster ==i], sep='', collapse=' ')
  centroids[i,]=colMeans(svd_ump[clus$cluster ==i,])
}

displayWords=vector()
for(i in 1:k){displayWords[i] = paste(top.words[[i]][1:7] , sep=' ', collapse='<br>')}
```

### Cluster Summarization {-}

To qualitatively evaluate our meta-clustering via centroids on the visualization, we'll create some naive cluster summaries to display in the plot using the eigenvector centralities of the graph induced by cosine similarity between sentences in the clusters. 

```{r message=F, warning=F}
# SummarySentences=vector()
# for(i in 1:k){
# t=cluster.docs[i]
# t = gsub("<br>", " ", t, fixed =T)
# # Change alternative sentence-ending punctuation to '.'
# t = gsub("\\?", ".", t)
# t = gsub("\\!", ".", t)
# # Split by sentence
# t2 = strsplit(t, ".",fixed=TRUE)
# corpus <- Corpus(VectorSource(as.data.frame(t2)[,1]))
# # Remove stop words, numbers and stem
# corpus = tm_map(corpus,removeWords,c(stopwords("en"),'reuter', 'dlrs', 'mln', 'said','will', 'year', 'compani','pct','corp'))
# corpus = tm_map(corpus,removeNumbers)
# corpus = tm_map(corpus,stemDocument)
# ########################################################################
# # Remove empty documents that appear after removal of stopwords and numbers
# td1 = TermDocumentMatrix(corpus)
# empty.cols = col_sums(td1)==0
# td1 = td1[,!empty.cols]
# raw=unlist(t2)[!empty.cols]
# if(length(empty.cols)>0){corpus = corpus[!empty.cols]}
# td = weightTfIdf(td1,normalize=T)
# tdm_norm = apply(td, 2, function(x){x/c(sqrt(t(x)%*%x))})
# tdm_norm = as(tdm_norm,"sparseMatrix")
# # Cosine similarity
# C=t(tdm_norm)%*%tdm_norm
# # Remove self loops
# C[C>0.9999999] = 0
# g = graph_from_adjacency_matrix(C, weighted = T)
# j=order(eigen_centrality(g)$vector,decreasing=T)
# SummarySentences[i] = paste(raw[j[1:5]], sep= ' ',collapse='<br>')
# }
# save(SummarySentences, file='docs/final_data_plots/SummarySentences.RData')
load('docs/final_data_plots/SummarySentences.RData')
```


```{r message=F, warning=F, eval=F}
# cen_clus = hdbscan(centroids, 3) # Down to 81 Clusters...Looks Pretty Good. 
# save(cen_clus,file='docs/final_data_plots/cen_clus.RData')
load('docs/final_data_plots/cen_clus.RData')
```





```{r message=F, warning=F, eval=F}

fig <- plot_ly(type = 'scatter', mode = 'markers')%>%
  add_trace(x = centroids[,1],
            y = centroids[,2],
            text = ~paste('Key Words:', displayWords,"<br>Cluster Number: ", cen_clus$cluster,
                          "<br>Summary Sentences: ", SummarySentences),
            color=factor(cen_clus$cluster),
            marker=list( opacity=0.6),
            showlegend = FALSE)
saveWidget(fig, "docs/Centroid_Clus.html")

```

[Full Page Visualization](https://shainarace.github.io/Reuters/Centroid_Clus.html)

 Now we just need a function that maps the new centroid clustering back to the original points. Essentially one line of code in R, thanks to subsetting functionality (final line of function `remapClusters` below) but with the minor problem that noise points create an extra cluster. We simply add the noise cluster to the vector as cluster number `k+1`, and give it a value of `0` similar to the noise points. This creates some real problems with noise points. 

_Additional thought_ (not implemented) leave the noise points IN and cluster them with the centroids. This is a good idea because it allows points that were previously labeled as noise to potentially join a cluster of nearby centroids. 

```{r message=F, warning=F}

remapClusters = function(cen_clus,clus){
  k = length(clus$cluster_scores)
  c=as.vector(clus$cluster)
  c[c==0]=k+1
  cc=as.vector(cen_clus$cluster)
  cc[k+1]=0
  new = cc[c]
  return(new)
  }
  
```
 
### Visualization of Refined Clusters {-}


```{r message=F, warning=F, eval=F}

newclusters = remapClusters(cen_clus, clus) 

fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = svd_ump[,1],
    y = svd_ump[,2],
    text = ~paste('Heading:', head ,"<br>Text: ", raw_text ,"<br>Original Cluster Number: ", clusters, "<br>Centroid Cluster Number:", newclusters ),
    hoverinfo = 'text',
    color = factor(newclusters),
    marker=list(opacity=0.6),
    showlegend = F
  )

# saveWidget(fig, "docs/All_centroid_refined_clusters.html")

```

[Full Page Visualization](https://shainarace.github.io/Reuters/All_centroid_refined_clusters.html)

# UMAP of Full Sparse Matrix in Python

We first write the sparse normalized tf-idf matrix to file:

```{r eval =F}
writeMM(tfidf_tdm, file='/Users/shaina/Library/Mobile Documents/com~apple~CloudDocs/Datasets and Code/reuters21578/tfidf_norm_tdm')
```

Then after installing umap-learn via 

```
git clone https://github.com/lmcinnes/umap
cd umap
pip install --user -r requirements.txt
python setup.py install --user
```

```{python eval = F}
import numpy as np
import scipy.sparse
import sympy
import sklearn.datasets
import sklearn.feature_extraction.text
import umap.umap_ as umap
import umap.plot
import matplotlib.pyplot as plt
import csv 

A = scipy.io.mmread('/Users/shaina/Library/Mobile Documents/com~apple~CloudDocs/Datasets and Code/reuters21578/tfidf_norm_tdm')
A=A.tolil()
A=A.transpose()
mapper = umap.umap_.UMAP(metric='cosine', random_state=42, low_memory=True).fit(A)
umap.plot.points(mapper, values=np.arange(19744), theme='viridis')
```

We then exported the layout in `mapper.embedding_` for exploration with our usual plot function:

```{python eval = F}
filename = "/Users/shaina/Library/Mobile Documents/com~apple~CloudDocs/Datasets and Code/reuters21578/UMAPofTFIDFsparse.csv"
    
# writing to csv file 
with open(filename, 'w') as csvfile: 
    csvwriter = csv.writer(csvfile) 
    csvwriter.writerows(mapper.embedding_)
```

```{r eval=F}
layout = read.csv("/Users/shaina/Library/Mobile Documents/com~apple~CloudDocs/Datasets and Code/reuters21578/UMAPofTFIDFsparse.csv", header=F)
fig <- plot_ly(type = 'scatter', mode = 'markers')
fig <- fig %>%
  add_trace(
    x = layout[,1],
    y = layout[,2],
    marker = list(color = 'green',opacity=0.6),
    showlegend = F
  )

#saveWidget(fig, file='docs/SparseDataIntoUMAP.html')
```

[Full Page Visualization](https://shainarace.github.io/Reuters/SparseDataIntoUMAP.html)