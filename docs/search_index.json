[["index.html", "21,578 Reuters’ Newswires Chapter 1 Grand Visualization 1.1 Prepare Data", " 21,578 Reuters’ Newswires Shaina Race Bennett, PhD Chapter 1 Grand Visualization ## Loading required package: Matrix ## Loading required package: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union 1.1 Prepare Data -1. Get datetimes from python script. 0. Copy text of raw documents to separate vector for visualization/results. 1. Make lower case, emove stop words + “Reuters”, punctuation, and numbers; No stemming was ultimately used. 3. Create binary term-document matrix to remove terms occurring in less than 5 documents. 5. Remove documents left with fewer than 10 words remaining. 6. Ready the datetime, topics, and heading information for visualization. #setwd(&#39;/Users/shaina/Library/Mobile Documents/com~apple~CloudDocs/Datasets and Code/reuters21578/&#39;) # load(&#39;Reuters.RData&#39;) # # Step 0 # # 1 # ############################################################ # R = Reuters21578 # R = tm_map(R,content_transformer(tolower)) # ############################################################ # # 2 # ############################################################ # R = tm_map(R,removeWords,stopwords(&quot;en&quot;)) # R = tm_map(R,removePunctuation) # R = tm_map(R,removeNumbers) # #R = tm_map(R,stemDocument) # R = tm_map(R,removeWords, c(&#39;reuters&#39;)) # ############################################################ # # 3 # ############################################################ # tdm = TermDocumentMatrix(R) # binary = weightBin(tdm) # ############################################################ # # 4 # ############################################################ # keep_terms = row_sums(binary)&gt;=5 # tdm = tdm[keep_terms,] # ############################################################ # # 5 # ############################################################ # keep_docs = col_sums(tdm)&gt;10 # R = R[keep_docs] # tdm = tdm[,keep_docs ] # dim(tdm) # length(R) # ############################################################ # datetime = read.csv(&#39;datetimes.csv&#39;) # datetime=datetime[keep_docs,] # head = unlist(meta(R,&quot;heading&quot;)) # ############################################################ # raw_data = unlist(content(Reuters21578[keep_docs])) # raw_text=raw_data[names(raw_data)==&#39;content&#39;] # raw_text = str_squish(raw_text) # head=raw_data[names(raw_data)==&#39;meta.heading&#39;] # head = str_squish(head) # lewissplit = raw_data[names(raw_data)==&#39;meta.lewissplit&#39;] # ############################################################ # # add breaks for text wrapping # ############################################################ # # raw_text = gsub(&quot;(.{60,}?)\\\\s&quot;, &quot;\\\\1&lt;br&gt;&quot;, raw_text) ############################################################ # Save data to avoid repeat processing ############################################################ # save(raw_text,head,lewissplit,tdm,R,datetime, file=&#39;processedV1.RData&#39;) load(&#39;processedV1.RData&#39;) "],["exploratory-analysis-via-svd.html", "Chapter 2 Exploratory Analysis via SVD", " Chapter 2 Exploratory Analysis via SVD # tfidf_tdm = weightTfIdf(tdm, normalize=T) # m = Matrix::sparseMatrix(i=tfidf_tdm$i, # j=tfidf_tdm$j, # x=tfidf_tdm$v, # dims=c(tfidf_tdm$nrow, tfidf_tdm$ncol), # dimnames = tfidf_tdm$dimnames) #svd = irlba(m, 150) #save(svd,file=&#39;svd.RData&#39;) load(&#39;svd.RData&#39;) df = data.frame(x=1:150,d=svd$d) g1 = ggplot(data=df, aes(x=x, y=d, group=1)) + geom_line(color=&quot;red&quot;)+labs(y=&#39;Singular Values&#39;,x=&#39;index&#39;, title=&#39;Screeplot of Reuters tf-idf Matrix, vlines at 10, 25&#39;) + geom_point() + geom_vline(xintercept = 25, linetype=&quot;dotted&quot;, color = &quot;blue&quot;, size=1) + geom_vline(xintercept = 10, linetype=&quot;dotted&quot;, color = &quot;blue&quot;, size=1) u.df = data.frame(x=svd$v[,1], y=svd$v[,2]) g2 = ggplot(data=u.df, aes(x=x, y=y)) + geom_point()+labs(y=&#39;Second Singular Component&#39;,x=&#39;First Singular Component&#39;, title=&#39;SVD Projection of Reuters tf-idf Term-Document Matrix&#39;) grid.arrange(g1,g2,ncol=1) "],["umap-projection-of-150-singular-vectors.html", "Chapter 3 UMAP Projection of 150 Singular Vectors", " Chapter 3 UMAP Projection of 150 Singular Vectors # svd_ump = umap(svd$v) # save(svd_ump, file=&#39;svd_ump.RData&#39;) load(&#39;svd_ump.RData&#39;) fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = svd_ump$layout[,1], y = svd_ump$layout[,2], text = ~paste(&#39;heading:&#39;, head ,&quot;$&lt;br&gt;text: &quot;, raw_text ), hoverinfo = &#39;text&#39;, marker = list(color=&#39;green&#39;), showlegend = F ) fig Outliers causing annoying viz issues requiring the zoom. We will routinely omit these outliers (after noting they make nice clusters of related documents) when creating the plot to avoid having to zoom on the main plot. index_subset = abs(svd_ump$layout[,1]) &lt;20 &amp; abs(svd_ump$layout[,2]) &lt;20 data_subset = svd_ump$layout[index_subset,] raw_text_subset = raw_text[index_subset] head_subset = head[index_subset] fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;heading:&#39;, head_subset ,&quot;$&lt;br&gt;text: &quot;, raw_text_subset ), hoverinfo = &#39;text&#39;, marker = list(color=&#39;green&#39;), showlegend = F ) fig After omitting the outliers, we see a nice plot that looks like it has some nice cluster separation. "],["use-hdbscan-to-cluster-umap-projection.html", "Chapter 4 Use HDBSCAN to cluster UMAP projection", " Chapter 4 Use HDBSCAN to cluster UMAP projection Two options here - can cluster this UMAP projection or can opt to cluster some higher-dimensional projection (like the singular vectors themselves) and see how that looks in the UMAP Space. UMAP clustering seems to perform really well, even better than singular vector input, so we stick with it. We define n as the number of documents and k as the number of clusters ### clus = hdbscan(svd$v[,1:25],10) ### save(clus,file=&#39;hdbscan_clusters10.RData&#39;) ### load(&#39;hdbscan_clusters10.RData&#39;) #clus = hdbscan(svd_ump$layout,5) # save(clus,file=&#39;alldocs_hdbscan_of_map5.RData&#39;) load(&#39;alldocs_hdbscan_of_map5.RData&#39;) (k = length(clus$cluster_scores)) ## [1] 733 (n=length(clus$cluster)) ## [1] 18519 "],["the-grand-visualization.html", "Chapter 5 The Grand Visualization", " Chapter 5 The Grand Visualization Full Page Rendering clusters = clus$cluster[index_subset] fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, color = clusters, showlegend = F ) fig #saveWidget(fig, &quot;All_clusters_noTopics_UMAPClus_wNoise.html&quot;) "],["prepare-top-words-cluster-documents.html", "Chapter 6 Prepare top words, Cluster documents", " Chapter 6 Prepare top words, Cluster documents Get top words for further visualization and pile all documents in a cluster into one giant document for the purposes of summarization. top.words=list() cluster.docs = vector() centroids = matrix(NA,k,2) mem=matrix(NA,nrow=n,ncol=k) for(i in 1:k){ mem[,i] = clus$cluster ==i tdmi = tdm[,mem[,i]] rs = row_sums(tdmi) top.words[[i]] = names(rs[order(rs,decreasing=T)])[1:10] cluster.docs[i] = paste(raw_text_subset[clus$cluster ==i], sep=&#39;&#39;, collapse=&#39; &#39;) centroids[i,]=colMeans(svd_ump$layout[clus$cluster ==i,]) } displayWords=vector() for(i in 1:k){displayWords[i] = paste(top.words[[i]][1:7] , sep=&#39; &#39;, collapse=&#39;&lt;br&gt;&#39;)} "],["omit-some-noise-points-for-more-cluster-clarity.html", "Chapter 7 Omit some noise points for more cluster clarity", " Chapter 7 Omit some noise points for more cluster clarity We could reduce the noise on the plot by omitting some of the points with high outlier scores, but generally I hate doing this because it can be a good way to accidently lose something you didn’t know you wanted. However, it could have it’s advantages as a strategy and the outlier_score of hdbscan() is a nice threshold to play with for further analytical paths. Full Page Rendering index_subset = abs(svd_ump$layout[,1]) &lt;20 &amp; abs(svd_ump$layout[,2]) &lt;20 &amp; clus$outlier_scores&lt;0.6 data_subset = svd_ump$layout[index_subset,] raw_text_subset = raw_text[index_subset] head_subset = head[index_subset] clusters = factor(clus$cluster[index_subset]) fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, color = clusters, showlegend = F ) fig ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors #saveWidget(fig, &quot;All_clusters_noTopics_UMAPClus.html&quot;) "],["cluster-refinement.html", "Chapter 8 Cluster Refinement 8.1 Refinement Idea 1: Clustering the centroids 8.2 Grand Visualization of Refined Clusters 8.3 Refinement Idea 1: Divide and Conquer", " Chapter 8 Cluster Refinement Not stoked that so many bigger clusters seem to get broken up even though they seem nicely separated on the UMAP projection. Would love to deal with the big center blob a bit better. Two ideas for potential refinement: Run the clustering again on centroids of clusters Focus on the blob and see if treating it separately helps - potentially less information overall to squeeze into the viz, allowing for more separation - divide and conquer. The first idea is easier so we’ll start there: 8.1 Refinement Idea 1: Clustering the centroids cen_clus = hdbscan(centroids, 3) # Down to 78 Clusters...Looks Pretty Good. # Omit the 2 outside fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;)%&gt;% add_trace(x = centroids[,1], y = centroids[,2], text = ~paste(&#39;Key Words:&#39;, displayWords,&quot;$&lt;br&gt;Cluster Number: &quot;, cen_clus$cluster ), color=factor(cen_clus$cluster), showlegend = FALSE) fig ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors Now we just need a function that maps the new centroid clustering back to the original points. Essentially one line of code in R, thanks to subsetting functionality (final line of function remapClusters below) but with the minor problem that noise points create an extra cluster. We simply add the noise cluster to the vector as cluster number k+1, and give it a value of 0 similar to the noise points. Additional thought (not implemented) leave the noise points IN and cluster them with the centroids. This is a good idea because it allows points that were previously labeled as noise to potentially join a cluster of nearby centroids. remapClusters = function(cen_clus,clus){ k = length(clus$cluster_scores) c=as.vector(clus$cluster) c[c==0]=k+1 cc=as.vector(cen_clus$cluster) cc[k+1]=0 new = cc[c] return(new) } 8.2 Grand Visualization of Refined Clusters Full Page Rendering newclusters = remapClusters(cen_clus, clus) newclusters = newclusters[index_subset] fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, color = factor(newclusters), showlegend = F ) fig ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors #saveWidget(fig, &quot;All_centroid_refined_clusters.html&quot;) Onto the next thought for refinement: divide and conquer. 8.3 Refinement Idea 1: Divide and Conquer Here, we divide the data according to UMAP and recompute the SVD of that subset. We see better cluster separation than we did in the corresponding rectangle on our original “Grand Viz”, which suggests this might be a viable line of attack. Full Page Rendering # Take rectangular subset on the interval x=y=[-2,2] index_subset2=abs(svd_ump$layout[,1]) &lt;2 &amp; abs(svd_ump$layout[,2]) &lt;2 tdm_subset = tdm[,index_subset2] tdm_subset = tdm_subset[row_sums(tdm_subset)!=0, ] tfidf_tdm_subset = weightTfIdf(tdm_subset, normalize=T) m = Matrix::sparseMatrix(i=tfidf_tdm_subset$i, j=tfidf_tdm_subset$j, x=tfidf_tdm_subset$v, dims=c(tfidf_tdm_subset$nrow, tfidf_tdm_subset$ncol), dimnames = tfidf_tdm_subset$dimnames) # Take SVD of the subset and compute the UMAP svd_subset = irlba(m,15) svd_subset_map = umap(svd_subset$v) # Subset raw text for visualization raw_text_subset2 = raw_text[index_subset2] head_subset2 = head[index_subset2] # Cluster clus2=hdbscan(svd_subset_map$layout,4) fig2 &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig2 &lt;- fig2 %&gt;% add_trace( x = svd_subset_map$layout[,1], y = svd_subset_map$layout[,2], text = ~paste(&#39;heading:&#39;, head_subset2 ,&quot;$&lt;br&gt;text: &quot;, raw_text_subset2,&quot;$&lt;br&gt;Cluster Number: &quot;, clus2$cluster ), hoverinfo = &#39;text&#39;, color=factor(clus2$cluster), showlegend = F ) fig2 ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): n too large, allowed maximum for palette Set2 is 8 ## Returning the palette you asked for with that many colors #saveWidget(fig2, &#39;Plots/SubsettingUMAPforRepeatSVD.html&#39;) "]]
