[["index.html", "21,578 Reuters’ Newswires 1 Prepare Data", " 21,578 Reuters’ Newswires Shaina Race Bennett, PhD 1 Prepare Data These 21,578 documents are readily available in an R document corpus in the package tm.corpus.Reuters21578. However, this corpus comes pre-loaded with many missing headings and mostly missing dates, so we read the files line-by-line to extract the date, text (with heading), and the value of lewissplit in case we wanted to perform some predictive modeling. This ended up being really annoying as the result of non-UTF-8 encoding on file 17 - but once that was fixed, we got what we needed. Beautiful Soup was no help here, it made something more like Angry Soup. # install.packages(&quot;tm.corpus.Reuters21578&quot;, repos = &quot;http://datacube.wu.ac.at&quot;) # install.packages(&quot;SnowballC&quot;) # install.packages(&quot;textreg&quot;) # install.packages(&#39;tm.corpus.Reuters21578&#39;) # install.packages(&#39;text2vec&#39;) # install.packages(&#39;Matrix&#39;) # install.packages(&#39;umap&#39;) # install.packages(&#39;tm&#39;) # install.packages(&#39;slam&#39;) # install.packages(&#39;irlba&#39;) # install.packages(&#39;dbscan&#39;) # install.packages(&#39;plotly&#39;) # install.packages(&#39;gridExtra&#39;) # install.packages(&#39;lubridate&#39;) # install.packages(&#39;maxmatching&#39;) # install.packages(&#39;plyr&#39;) # install.packages(&#39;rARPACK&#39;) # install.packages(&#39;textrank&#39;) # install.packages(&#39;rvest&#39;) # install.packages(&#39;tidytext&#39;) # install.packages(&#39;tsne&#39;) ################################################################## library(textrank) library(rvest) library(tidytext) library(dbscan) library(ggplot2) library(irlba) library(umap) library(slam) library(text2vec) #library(tm.corpus.Reuters21578) library(SnowballC) library(tm) library(textreg) library(stringr) library(Matrix) library(plotly) library(gridExtra) library(lubridate) library(rARPACK) library(htmlwidgets) library(bookdown) library(stringr) library(fs) library(tsne) #setwd(&#39;/Users/shaina/Library/Mobile Documents/com~apple~Clouddocs/final_data_plots/Datasets and Code/reuters21578/&#39;) # PATH = &#39;/Users/shaina/Library/Mobile Documents/com~apple~Clouddocs/final_data_plots/Datasets and Code/reuters21578/Files/&#39; # filenames=dir_ls(PATH) # datetime=vector() # text = vector() # lewissplit=vector() # # # for(file in filenames){ # con = file(file,&quot;r&quot;, encoding = &quot;UTF-8&quot;) # line = readLines(con, encoding=&quot;UTF-8&quot;) # line = paste(line, sep=&#39; &#39;, collapse = &#39; &#39;) # # String size fixed # lewis_idx=str_locate_all(pattern=&#39;LEWISSPLIT=&#39;,line) # date_idx=str_locate_all(pattern=&#39;&lt;DATE&gt;&#39;,line) # for(i in 1:nrow(lewis_idx[[1]])){ # datetime[length(datetime)+1]=substr(line, date_idx[[1]][i,2]+1,date_idx[[1]][i,2]+20) # lewissplit[length(lewissplit)+1]=substr(line, lewis_idx[[1]][i,2]+2,lewis_idx[[1]][i,2]+5) # } # # String size not fixed # line = paste(line, sep=&#39; &#39;, collapse = &#39; &#39;) # line=gsub(&#39;&lt;/&#39;,&#39;&lt;&#39;,line) # text_idx=str_locate_all(pattern=&#39;&lt;TEXT&#39;,line) # for(i in seq(1,nrow(text_idx[[1]])-1,2)){ # text[length(text)+1]=substr(line, text_idx[[1]][i,2]+2,text_idx[[1]][i+1,1]-1) # } # close(con) # } # # # Annoying that I end up with fewer documents than I should. Investigate that. # # # Explore why I end up with fewer documents than I should by counting occurrences of &lt;TEXT on each file: &lt;aha&gt; # Fixed UTF-8 encoding on file 17 and now it works. # date = as.POSIXct(datetime,format = &#39;%d-%b-%Y %H:%M:%S&#39;) # datetime[which(is.na(date))] # datetime[which(is.na(date))] = c(&quot;5-APR-1987 01:53:30&quot;, &quot;31-MAR-1987 05:12:1&quot;) # datetime = as.POSIXct(datetime,format = &#39;%d-%b-%Y %H:%M:%S&#39;) # lewissplit[lewissplit==&#39;TRAI&#39;]=&#39;train&#39; # lewissplit[lewissplit==&#39;TEST&#39;]=&#39;test&#39; # # ############################################################################ # # Now I can pull out the heading... # ############################################################################ # title_idx = str_locate_all(pattern=&#39;&lt;TITLE&gt;&#39;,text) # head=vector() # for(i in 1:(length(text))){ # if(nrow(title_idx[[i]])&lt;2){ # head[i]=&#39;&#39; # }else{ # head[i]=substr(text[i], title_idx[[i]][1,2]+1,title_idx[[i]][2,1]-1) # } # } # head=gsub(&#39;&amp;lt;&#39;,&#39;&lt;&#39;,head) # ############################################################################ # # ...and the raw article text. # ############################################################################ # body_idx = str_locate_all(pattern=&#39;&lt;BODY&gt;&#39;,text) # body=vector() # for(i in 1:(length(text))){ # if(nrow(body_idx[[i]])&lt;2){ # body[i]=text[i] # }else{ # body[i]=substr(text[i], body_idx[[i]][1,2]+1,body_idx[[i]][2,1]-1) # } # } # text=body # Reuters &lt;- Corpus(VectorSource(text)) #save(text,head,Reuters,lewissplit,datetime,file=&#39;RawDataRead.RData&#39;) load(&#39;docs/final_data_plots/RawDataRead.RData&#39;) -1. Get datetimes from python script. 0. Copy text of raw documents to separate vector for visualization/results. 1. Make lower case, emove stop words + “Reuters”, punctuation, and numbers; No stemming was ultimately used. 3. Create binary term-document matrix to remove terms occurring in less than 5 documents. 5. Remove documents left with fewer than 10 words remaining. 6. Ready the datetime, topics, and heading information for visualization. # load(&#39;Reuters.RData&#39;) # # Step 0 # # 1 # ############################################################ # R = Reuters # R = tm_map(R,content_transformer(tolower)) # # ############################################################ # # # 2 # # ############################################################ # R = tm_map(R,removeWords,stopwords(&quot;en&quot;)) # R = tm_map(R,removePunctuation) # R = tm_map(R,removeNumbers) # R = tm_map(R,stemDocument) # R = tm_map(R,removeWords, c(&#39;reuter&#39;, &#39;dlrs&#39;, &#39;mln&#39;, &#39;said&#39;,&#39;will&#39;, &#39;year&#39;, &#39;compani&#39;,&#39;pct&#39;,&#39;corp&#39; )) # # ############################################################ # # # 3 # # ############################################################ # tdm = TermDocumentMatrix(R) # binary = weightBin(tdm) # # ############################################################ # # # 4 # # ############################################################ # keep_terms = row_sums(binary)&gt;=5 # tdm = tdm[keep_terms,] # # ############################################################ # # # 5 # # ############################################################ # keep_docs = col_sums(tdm)&gt;10 # R = R[keep_docs] # tdm = tdm[,keep_docs ] # dim(tdm) # length(R) # datetime = datetime[keep_docs] # lewissplit=lewissplit[keep_docs] # head=head[keep_docs] # raw_text=text[keep_docs] # ############################################################# # # add breaks for text wrapping # ############################################################# # raw_text = gsub(&quot;(.{60,}?)\\\\s&quot;, &quot;\\\\1&lt;br&gt;&quot;, raw_text) ############################################################ # Save data to avoid repeat processing ############################################################ #save(raw_text,head,lewissplit,tdm,R,datetime, file=&#39;processedV2.RData&#39;) load(&#39;docs/final_data_plots/processedV2.RData&#39;) "],["exploratory-analysis.html", "2 Exploratory Analysis 2.1 Term Frequencies", " 2 Exploratory Analysis 2.1 Term Frequencies This initial exploration of term frequencies allowed us to find some extraordinarily common words for this corpus that were added to the stop list. Not all common words were added to the stop list, however. Some common words may still have found relevance in determining certain groups of documents (an example would be the abbreviation ‘vs’ which indicates a comparison, most notably from an earnings report.) Documents per word bin = weightBin(tdm) df=data.frame(docFreqs = row_sums(bin)) g1 = ggplot(df,aes(x=docFreqs)) + geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) + labs(x = &quot;Number of Documents in which \\n a Word Appears&quot;) df=data.frame(docFreqs=df$docFreqs[df$docFreqs&lt;100] ) g2 = ggplot(df,aes(x=docFreqs)) + geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) + labs(x = &quot;Number of Documents in which \\n a Word Appears&quot;, y=&#39;Frequency&#39;, title=&#39;Same Distribution Cut at x=100&#39;) grid.arrange(g1,g2,ncol=2) TF-IDF per word tfi = weightTfIdf(tdm) df = data.frame(termFreqs = row_sums(tfi)) g1 = ggplot(df,aes(x=termFreqs)) + geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) + labs(x = &quot;Sum of TF-IDF Weights \\n for each words&quot;) df=data.frame(termFreqs=df$termFreqs[df$termFreqs&lt;100] ) g2 = ggplot(df,aes(x=termFreqs)) + geom_histogram(aes(y=..density..), alpha=0.5) + geom_density( alpha = 0.2) + labs(x = &quot;Sum of TF-IDF Weights \\n for each word&quot;, y=&#39;Frequency&#39;, title=&#39;Same Distribution Cut at x=100&#39;) grid.arrange(g1,g2,ncol=2) ## via SVD # tfidf_tdm = weightTfIdf(tdm, normalize=T) # m = Matrix::sparseMatrix(i=tfidf_tdm$i, # j=tfidf_tdm$j, # x=tfidf_tdm$v, # dims=c(tfidf_tdm$nrow, tfidf_tdm$ncol), # dimnames = tfidf_tdm$dimnames) # svd = irlba(m, 150) # save(svd,file=&#39;svd.RData&#39;) load(&#39;docs/final_data_plots/svd.RData&#39;) df = data.frame(x=1:150,d=svd$d) g1 = ggplot(data=df, aes(x=x, y=d, group=1)) + geom_line(color=&quot;red&quot;)+labs(y=&#39;Singular Values&#39;,x=&#39;index&#39;, title=&#39;Screeplot of Reuters tf-idf Matrix, vlines at 10, 25&#39;) + geom_point() + geom_vline(xintercept = 25, linetype=&quot;dotted&quot;, color = &quot;blue&quot;, size=1) + geom_vline(xintercept = 10, linetype=&quot;dotted&quot;, color = &quot;blue&quot;, size=1) u.df = data.frame(x=svd$v[,1], y=svd$v[,2]) g2 = ggplot(data=u.df, aes(x=x, y=y)) + geom_point()+labs(y=&#39;Second Singular Component&#39;,x=&#39;First Singular Component&#39;, title=&#39;SVD Projection of Reuters tf-idf Term-Document Matrix&#39;) g1 fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = svd$v[,1], y = svd$v[,2], text = ~paste(&#39;heading:&#39;, head ,&quot;$&lt;br&gt;text: &quot;, raw_text ), hoverinfo = &#39;text&#39;, marker = list(color=&#39;green&#39;, opacity=0.6), showlegend = F ) fig Our initial Creation of this SVD Projection allowed us to see that we had an issue with briefs vs. articles in this data. "],["umap.html", "3 UMAP", " 3 UMAP We’ll use the mathemagical Uniform Manifold Approximation and Projection (UMAP) algorithm to project the already dimension-reduced data (150 singular vectors) into 2-space. UMAP is a dimension reduction technique that builds on the notion neighbor graphs with ideas from topology. It is similar to t-SNE in its approach, but the fundamentals are based on firmer (and more complicated) mathematical theory (manifolds/topology). # svd_ump = umap(svd$v) # save(svd_ump, file=&#39;svd_ump.RData&#39;) load(&#39;docs/final_data_plots/svd_ump.RData&#39;) fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = svd_ump$layout[,1], y = svd_ump$layout[,2], text = ~paste(&#39;heading:&#39;, head ,&quot;$&lt;br&gt;text: &quot;, raw_text ), hoverinfo = &#39;text&#39;, marker = list(color=&#39;green&#39;, opacity=0.6), showlegend = F ) fig Outliers causing annoying viz issues requiring the zoom. We will routinely omit these outliers (after noting they make nice clusters of related documents) when creating the plot to avoid having to zoom on the main plot. index_subset = abs(svd_ump$layout[,1]) &lt;20 &amp; abs(svd_ump$layout[,2]) &lt;20 data_subset = svd_ump$layout[index_subset,] raw_text_subset = raw_text[index_subset] head_subset = head[index_subset] fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;heading:&#39;, head_subset ,&quot;$&lt;br&gt;text: &quot;, raw_text_subset ), hoverinfo = &#39;text&#39;, marker = list(color=&#39;green&#39;), showlegend = F ) fig After omitting the straggler points on the outskirts, we see a nice plot that looks like it has some nice cluster separation. "],["clustering-hdbscan.html", "4 Clustering: HDBSCAN 4.1 Prepare top words, Cluster documents", " 4 Clustering: HDBSCAN Two options here - can cluster this UMAP projection or can opt to cluster some higher-dimensional projection (like the singular vectors themselves) and see how that looks in the UMAP Space. UMAP clustering seems to perform really well, even better than singular vector input, so we stick with it. We define n as the number of documents and k as the number of clusters. Hierarchical DBSCAN is a fast algorithm that adapts the ideas of single linkage clustering (minimal spanning trees) to DBSCAN (density based spatial clustering of applications with noise) to create a hierarchical map of density based clusters. ### clus = hdbscan(svd$v[,1:25],10) ### save(clus,file=&#39;hdbscan_clusters10.RData&#39;) ### load(&#39;hdbscan_clusters10.RData&#39;) #clus = hdbscan(svd_ump$layout,5) # save(clus,file=&#39;alldocs_hdbscan_of_map5.RData&#39;) load(&#39;docs/final_data_plots/alldocs_hdbscan_of_map5.RData&#39;) n=length(clus$cluster) (k = length(clus$cluster_scores)) ## [1] 772 We get a LOT of clusters from hdbscan - this makes sense, there is a lot going on in this corpus! But it might be nice to refine those clusters so that we can see which ones are related. We’ll get to that after we explore this great visualization. 4.1 Prepare top words, Cluster documents Get top words for further visualization and pile all documents in a cluster into one giant document for the purposes of summarization. top.words=list() cluster.docs = vector() centroids = matrix(NA,k,2) mem=matrix(NA,nrow=n,ncol=k) for(i in 1:k){ mem[,i] = clus$cluster ==i tdmi = tdm[,mem[,i]] rs = row_sums(tdmi) top.words[[i]] = names(rs[order(rs,decreasing=T)])[1:10] cluster.docs[i] = paste(raw_text_subset[clus$cluster ==i], sep=&#39;&#39;, collapse=&#39; &#39;) centroids[i,]=colMeans(svd_ump$layout[clus$cluster ==i,]) } displayWords=vector() for(i in 1:k){displayWords[i] = paste(top.words[[i]][1:7] , sep=&#39; &#39;, collapse=&#39;&lt;br&gt;&#39;)} "],["the-grand-visualization.html", "5 The Grand Visualization 5.1 Omit some noise points for more cluster clarity", " 5 The Grand Visualization Full Page Rendering Note: We don’t have enough colors! The colors are recycled but hopefully will still help. Cluster numbers in tooltip for certainty clusters = factor(clus$cluster[index_subset]) fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, color=clusters, marker = list( opacity=0.6), showlegend = F ) fig #saveWidget(fig, &quot;All_clusters_noTopics_UMAPClus_wNoise.html&quot;) 5.1 Omit some noise points for more cluster clarity We could reduce the noise on the plot by omitting some of the points with high outlier scores, but generally I hate doing this because it can be a good way to accidently lose something you didn’t know you wanted. However, it could have it’s advantages as a strategy and the outlier_score of hdbscan() is a nice threshold to play with for further analytical paths. Full Page Rendering index_subset = abs(svd_ump$layout[,1]) &lt;20 &amp; abs(svd_ump$layout[,2]) &lt;20 &amp; clus$outlier_scores&lt;0.6 data_subset = svd_ump$layout[index_subset,] raw_text_subset = raw_text[index_subset] head_subset = head[index_subset] clusters = factor(clus$cluster[index_subset]) fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, markers = list(opacity=0.6), color = clusters, showlegend = F ) fig #saveWidget(fig, &quot;All_clusters_noTopics_UMAPClus.html&quot;) "],["cluster-refinement.html", "6 Cluster Refinement 6.1 Refinement Idea 1: Clustering the centroids 6.2 Refinement Idea 1: Divide and Conquer", " 6 Cluster Refinement Problem: Many medium-big clusters get broken up even though they seem nicely separated on the UMAP projection. Would love to deal with the big center blob a bit better. Two ideas for potential refinement: Run the clustering again on centroids of clusters Focus on the blob and see if treating it separately helps - potentially less information overall to squeeze into the viz, allowing for more separation - divide and conquer. The first idea is easier so we’ll start there: 6.1 Refinement Idea 1: Clustering the centroids cen_clus = hdbscan(centroids, 3) # Down to 78 Clusters...Looks Pretty Good. # Omit the 2 outside fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;)%&gt;% add_trace(x = centroids[,1], y = centroids[,2], text = ~paste(&#39;Key Words:&#39;, displayWords,&quot;$&lt;br&gt;Cluster Number: &quot;, cen_clus$cluster ), color=factor(cen_clus$cluster), marker=list( opacity=0.6), showlegend = FALSE) fig Now we just need a function that maps the new centroid clustering back to the original points. Essentially one line of code in R, thanks to subsetting functionality (final line of function remapClusters below) but with the minor problem that noise points create an extra cluster. We simply add the noise cluster to the vector as cluster number k+1, and give it a value of 0 similar to the noise points. Additional thought (not implemented) leave the noise points IN and cluster them with the centroids. This is a good idea because it allows points that were previously labeled as noise to potentially join a cluster of nearby centroids. remapClusters = function(cen_clus,clus){ k = length(clus$cluster_scores) c=as.vector(clus$cluster) c[c==0]=k+1 cc=as.vector(cen_clus$cluster) cc[k+1]=0 new = cc[c] return(new) } Grand Visualization of Refined Clusters Full Page Rendering newclusters = remapClusters(cen_clus, clus) newclusters = newclusters[index_subset] fig &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig &lt;- fig %&gt;% add_trace( x = data_subset[,1], y = data_subset[,2], text = ~paste(&#39;Heading:&#39;, head_subset ,&quot;$&lt;br&gt;Text: &quot;, raw_text_subset ,&quot;$&lt;br&gt;Cluster Number: &quot;, clusters), hoverinfo = &#39;text&#39;, color = factor(newclusters), marker=list(opacity=0.6), showlegend = F ) fig #saveWidget(fig, &quot;All_centroid_refined_clusters.html&quot;) Onto the next thought for refinement: divide and conquer. 6.2 Refinement Idea 1: Divide and Conquer Here, we divide the data according to UMAP and recompute the SVD of that subset. We see better cluster separation than we did in the corresponding rectangle on our original “Grand Viz”, which suggests this might be a viable line of attack. Full Page Rendering # Take rectangular subset on the interval x=y=[-2,2] index_subset2=abs(svd_ump$layout[,1]) &lt;2 &amp; abs(svd_ump$layout[,2]) &lt;2 tdm_subset = tdm[,index_subset2] tdm_subset = tdm_subset[row_sums(tdm_subset)!=0, ] tfidf_tdm_subset = weightTfIdf(tdm_subset, normalize=T) m = Matrix::sparseMatrix(i=tfidf_tdm_subset$i, j=tfidf_tdm_subset$j, x=tfidf_tdm_subset$v, dims=c(tfidf_tdm_subset$nrow, tfidf_tdm_subset$ncol), dimnames = tfidf_tdm_subset$dimnames) # Take SVD of the subset and compute the UMAP svd_subset = irlba(m,15) svd_subset_map = umap(svd_subset$v) # Subset raw text for visualization raw_text_subset2 = raw_text[index_subset2] head_subset2 = head[index_subset2] # Cluster clus2=hdbscan(svd_subset_map$layout,4) fig2 &lt;- plot_ly(type = &#39;scatter&#39;, mode = &#39;markers&#39;) fig2 &lt;- fig2 %&gt;% add_trace( x = svd_subset_map$layout[,1], y = svd_subset_map$layout[,2], text = ~paste(&#39;heading:&#39;, head_subset2 ,&quot;$&lt;br&gt;text: &quot;, raw_text_subset2,&quot;$&lt;br&gt;Cluster Number: &quot;, clus2$cluster ), hoverinfo = &#39;text&#39;, color=factor(clus2$cluster), showlegend = F ) fig2 #saveWidget(fig2, &#39;Plots/SubsettingUMAPforRepeatSVD.html&#39;) "]]
